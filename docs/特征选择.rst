===============================
特征选择 Feature Selection
===============================

.. contents:: :local:

1. 基本概念
======================

1.1 什么是特征选择？
--------------------------------------
**Feature selection** is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. (from `Wiki <https://en.wikipedia.org/wiki/Feature_selection>`__  )

1.2 为什么要做特征选择？
--------------------------------------

 - 在特征很多的情况下，模型学习容易遇到问题，例如 `Curse of Dimensionality <https://en.wikipedia.org/wiki/Curse_of_dimensionality>`__ 
 - 简单的模型更便于解释
 - 降低了过拟合的风险
 - 缩短训练时间
 - 降低了因为数据出错导致问题的风险
 - 很多时候多个特征之间是冗余的

2. Filter 方法
======================
2.1 综述
--------------------------------------
**特性**
 - 与具体算法无关，结果完全取决于单个特征本身
 - 计算量少
 - 很少单独使用，通常只作为初步筛选

**步骤**
 - 根据某一标准对特征打分并排序
 - 选择得分最高的Top N 个特征



3. Wrapper 方法
======================
3.1 综述
----------------------------------
**特性**
 - 利用某个具体的模型来对特征集打分
 - 对于每一个特征子集都会训练一个模型
 - 计算量大
 - 对于给定的具体算法，能产出最优表现的特征子集
 - 但对于不同的算法，结果不能通用


4. Embeddded 方法
======================

4.1 综述
------------------
**特性**
 - 在模型训练过程中同时做特征筛选
 - 相比 Wrapper 计算量更少，因为只训练一个模型


4.2 LASSO
--------------------------------------

4.3 基于决策树的特征重要性
--------------------------------------

4.4 回归系数
--------------------------------------

5. 其他方法
======================

5.1 Recursive Feature Elimination
--------------------------------------