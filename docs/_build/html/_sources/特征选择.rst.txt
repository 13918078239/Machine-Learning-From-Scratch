===============================
特征选择 Feature Selection
===============================

.. contents:: :local:

1. 基本概念
======================

1.1 什么是特征选择？
--------------------------------------
**Feature selection** is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. (from `Wiki <https://en.wikipedia.org/wiki/Feature_selection>`__  )

特征选择通常是紧跟着特征工程的后一步，即所有特征已经是干净的、经过适当的转换、编码，可以直接被算法使用的状态。

1.2 为什么要做特征选择？
--------------------------------------

 - 在特征很多的情况下，模型学习容易遇到问题，例如 `Curse of Dimensionality <https://en.wikipedia.org/wiki/Curse_of_dimensionality>`__ 
 - 简单的模型更便于解释
 - 降低了过拟合的风险
 - 缩短训练时间
 - 降低了因为数据出错导致问题的风险
 - 很多时候多个特征之间是冗余的

1.3 特征选择前是否先要分割训练集/测试集？
-------------------------------------------------
答案是：是的。必须是。

先分割好训练集/测试集的目地是防止 overfitting。这个道理很简单。测试集存在的本身就是为了完全独立于整个模型建立过程之外，所有会影响到模型学习的步骤，包括特征工程、特征清洗、特征选择等都应该只在训练集上进行，然后 Apply 到测试集上，再进行训练和测试。

很多百度上的答案和书本没有强调这一点，甚至给出错误的指示。想深入了解的朋友可以谷歌 “Data Leakage”。 这篇文章里也有比较正式的解答: `Data Leakage in Machine Learning <https://machinelearningmastery.com/data-leakage-machine-learning/>`__ 。 这里也有一个很好的讨论: `Should feature selection be performed only on training data <https://stats.stackexchange.com/questions/64825/should-feature-selection-be-performed-only-on-training-data-or-all-data>`__ 。


2. Filter 方法
======================
2.1 综述
--------------------------------------
**特性**
 - 与具体算法无关，结果完全取决于单个特征本身
 - 计算量少
 - 很少单独使用，通常只作为初步筛选

**步骤**
 - 根据某一标准对特征打分并排序
 - 选择得分最高的Top N 个特征

2.2 常量特征 Constant Features
--------------------------------------
如果某个特征在所有样本上都是同一个值，那它不会对模型的学习有任何作用，可以直接删除。

2.3 准常量特征 Quasi-Constant Features
------------------------------------------
如果某个特征在绝大部分样本上是同一个值，同样，很有可能也是无用。当然也有例外，特别是目标变量也是严重不平衡的时候，需要单独检查一下特征与目标变量的关系后再决定是否删除。

2.4 重复的特征
--------------------------------------
即两个特征在数据集上完全一样。当有多个类别变量并且做了 one-hot encoding 时，出现两个完全一致的特征的情况容易发生。这时移除其中任何一个即可。

2.5 彼此相关的特征
--------------------------------------
基于特征的相关性来筛选特征，建立在这个假设下： "Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other". 好的特征集应该彼此之间尽量不相关，而与目标变量高度相关。

通常，特征选择前已经做好了相应了特征工程工作，所有类别变量也已经编码成了数字形式，因此我们可以统一用 Pearson  相关系数来进行检查，找到彼此高度相关的特征集合，决定保留哪一些。Pearson 相关系数的结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。一般相关系数绝对值 >=0.8 的认为是高度相关。

但 Pearson 相关系数的一个明显缺陷是只对线性关系敏感。如果变量间的关系是非线性的，即使完全一一对应，Pearson相关性也可能在0左右。

2.6 基于统计量的特征排序
--------------------------

.. warning::

    以下几个方法采用某一个统计评估指标，单独的对每一个候选特征与目标变量直接的关系进行衡量和排序，根据排序结果选择靠前的特征。但问题是，这类方法不会考虑特征之间的相互作用，例如有些特征单个的看与目标变量毫无预测能力，但和其他特征组合在一起就有了一定效果。因此这一部分的方法需要谨慎使用。

2.6.1 互信息 Mutual Information
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
信息的概念来自于信息熵,是变量间相互依赖性的量度。通俗的来讲：互信息是一个随机变量包含另外一个随机变量的信息量，或者说如果已知一个变量，另外一个变量减少的信息量。

互信息的概念和计算 TODO

2.6.2 卡方检验
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

衡量两个类别变量样本的独立性。假设是：一个样本中已发生事件的次数分配会遵守某个特定的理论分配。通常的讲：观测频数跟实际频数应该没有区别，除非两个变量不是独立的。需要注意的是，当数据集很大时，大部分特征的卡方检验都会显著。真正使用的时候需要格外注意。

2.6.3 单变量 ROC-AUC 检验
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 - 用单个候选特征和目标变量建立一颗决策树
 - 根据模型的 ROC-AUC 得分（或MSE）对特征排序
 - 挑选排名最高的N个特征

2.6.4 IV 值
^^^^^^^^^^^^^^^^^
衡量两个类别变量的影响程度。IV 和互信息都源于信息论，这两个指标的关系很紧密。

3. Wrapper 方法
======================
3.1 综述
----------------------------------
**特性**
 - 利用某个具体的模型来对特征集打分
 - 对于每一个特征子集都会训练一个模型
 - 计算量大
 - 对于给定的具体算法，能产出最优表现的特征子集
 - 但对于不同的算法，结果不能通用

**步骤**
 - 搜索一个特征子集
 - 在该子集上训练一个模型
 - 评估模型表现
 - 重复以上步骤，直到满足某停止条件


**特征子空间搜索算法分类**

.. image:: images/Feature_selection.jpg

下面仅就其中几种基本方法稍作介绍。

3.2 Step Forward Selection
-------------------------------
**前向选择**

特征子集X从空集开始，每次选择一个特征x加入特征子集X，使得模型的表现指标最优（比如 ROC-AUC）。简单说就是，每次都选择一个使得评价函数的取值达到最优的特征加入，是一种简单的贪心算法。

缺点是只能加入特征而不能去除特征。例如：特征A完全依赖于特征B与C，可以认为如果加入了特征B与C则A就是多余的。假设序列前向选择算法首先将A加入特征集，然后又将B与C加入，那么特征子集中就包含了多余的特征A。


3.3 Step Backwards Selection
-------------------------------
**后向选择**

从特征全集O开始，每次从特征集O中剔除一个特征x，使得剔除特征x后评价函数值达到最优。

后向选择与前向选择正好相反，它的缺点是特征只能去除不能加入。以上两种都属于贪心算法，容易陷入局部最优值。

3.4 Exhaustive Feature Selection
--------------------------------------

暴力方法，遍历所有可能的特征子集，寻找评价函数最优的那一个子集。计算量非常大。例如名假设只有4个候选特征的情况下，就需要衡量所有一个特征、两个特征、三个特征、四个特征的组合（共4+6+4+1=15种）。在特征很多时需要耗费大量时间。

4. Embeddded 方法
======================

4.1 综述
------------------
**特性**
 - 在模型训练过程中同时做特征筛选
 - 相比 Wrapper 计算量更少，因为只训练一个模型
 - 能筛选出最适合于该算法的特征子集

**步骤**
 - 训练一个模型
 - 生成特征重要性
 - 丢弃不重要的特征


4.2 LASSO
--------------------------------------

4.3 基于决策树的特征重要性
--------------------------------------

4.3.1 基于随机森林的特征选择
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

随机森林具有准确率高，不容易过拟合等优点，同时还提供了一种有效的特征选择方法。在 Scikit-learn 中，随机森林输出的 feature importance 是根据平均不纯度的降低来计算的。在每个节点处，计算该节点使用的分割特征降低了多少树的不纯度（一般用 Gini系数衡量），然后再用到达该节点的样本个数加权（衡量多少比例的样本会被这个特征影响到最终的判断），在所有的树上平均后得到该特征的重要性。

需要注意的是，彼此相关的变量对于决策树来说，任何一个都可能被认为是重要特征，选择了其中一个，其他的关联特征的重要性就会下降。在使用随机森林做特征筛选之前建议先剔除彼此相关的特征。

另外，利用随机森林输出特征重要性的计算方法有很多，例如基于模型准确率的影响来衡量特征重要性。Scikit-learn 里的度量只是一种方法，更多方法可以参考  `Variable Selection using Random Forest <https://hal.archives-ouvertes.fr/file/index/docid/755489/filename/PRLv4.pdf>`__ 。


4.3.2 Recursive feature selection using random forests 
--------------------------------------------------------

鉴于上一节提到的随机森林面对相关特征，输出的特征重要性有偏差的弱点，除了先行剔除相关的特征外，还可以通过递归的方式进行筛选。基本的过程如下：

 - 使用所有特征训练随机森林
 - 移除最不重要的一个特征
 - 重新训练随机森林，计算特征重要性
 - 重复上述步骤，直到满足停止条件

4.3.3 Gradient Boosted trees importance
--------------------------------------------------------

与随机森林原理类似。

4.3.4 Extra-Tree importance
----------------------------------------------

与随机森林原理类似。


4.4 回归系数
--------------------------------------
TODO

5. 混合方法
======================

5.1 递归特征消除 Recursive Feature Elimination
----------------------------------------------------

 - 根据某个 Embedded 中的方法（例如随机森林、LASSO、回归系数）对特征重要性排序
 - 将最不重要的一个特征剔除，利用剩余所有特征再训练
 - 计算当前模型的评估指标（ROC-AUC/MSE 或其他）
 - 如果评估指标的下降超过了设定的阈值，则说明去除的特征很重要应当保留，反之则可以放心剔除该排名最末的特征
 - 重复以上步骤，直到剔除的特征会严重影响模型的表现，或者所有特征都被剔除了（说明你的数据集完全是垃圾）

 这个方法看起来和之前 Wrapper 中的后向选择有点类似，最大的不同是在选择删除哪个特征时，后者会评估所有特征的剔除对模型表现的影响，而前者只会根据特征重要性排名删除最末的一个特征。

5.2 递归特征增加 Recursive Feature Addition
----------------------------------------------------

 - 根据某个 Embedded 中的方法（例如随机森林、LASSO、回归系数）对特征重要性排序
 - 将最重要的一个特征加入模型进行训练，计算当前模型的评估指标（ROC-AUC/MSE 或其他）
 - 再次加入重要性最高的另一个特征，利用该特征以及之前模型已有的所有特征训练模型，计算模型评估指标
 - 如果评估指标的上升超过了设定的阈值，则说明增加的特征很有用应当保留，反之则剔除该特征
 - 重复以上步骤，直到所有的特征都被评估完

5.3 Feature Shuffling
-----------------------------
随机打乱一个特征的值，观察在算法上的表现是否会有明显下降。如果某个特征很重要，有很强的预测能力，那么打乱其顺序会造成模型表现的大幅度下降。反过来，如果某个特征无关紧要，那么对它怎么乱动，都不会影响最终的结果。


总结：
美团机器学习 P33 

.. https://stats.stackexchange.com/questions/81659/mutual-information-versus-correlation
.. https://www.quora.com/What-is-the-difference-between-mutual-information-and-correlation

.. https://www.cnblogs.com/heaad/archive/2011/01/02/1924088.html